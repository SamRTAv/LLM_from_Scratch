{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7S5eEBDo-Xg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Tokens\n"
      ],
      "metadata": {
        "id": "DfVxvbE3qYjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/Satvik-jain/DeepLearning_LSTM_WordPredictor/refs/heads/main/The%20Verdict%20-%20Next%20Word%20Predictor/data/Data.txt\"\n",
        "#so before importing the text directly from the github, got to github, click on raw and then copy the url in the url bar\n",
        "import requests\n",
        "r = requests.get(url)\n",
        "book = r.text"
      ],
      "metadata": {
        "id": "aq2-3HJFqagd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(book)) #printing the length of text\n",
        "print(book[450:560])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnTIE0ZNtBoO",
        "outputId": "8f3a3b2d-5fb8-4374-8b41-83e137b57816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20482\n",
            "oring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qMLhDV3FtO16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Our goal is to tokenize this 20,479-character short story into individual words and special characters that we can then turn into embeddings for LLM training\n",
        "#####How can we best split this text to obtain a list of tokens? For this, we go on a small excursion and use Python's regular expression library re for illustration purposes. (Note that you don't have to learn or memorize any regular expression syntax since we will transition to a pre-built tokenizer later in this chapter.)"
      ],
      "metadata": {
        "id": "rRk0ypgftg8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"hello everyone my name is something, an I. Finding it good\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "print(result)\n",
        "#we have to look forwward to spaces as well because they do play a role in sentences\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouuFEeTetmyC",
        "outputId": "4b5b003c-d0b1-4551-a4c8-6beefe8b1a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', ' ', 'everyone', ' ', 'my', ' ', 'name', ' ', 'is', ' ', 'something,', ' ', 'an', ' ', 'I.', ' ', 'Finding', ' ', 'it', ' ', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####So basically what is beign don eabove is that we are splitting the text wrt to whitespaces(\\s) , further we can split based on , . \\s or anything we feel should be included"
      ],
      "metadata": {
        "id": "WjxtbFi7uZT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.]|\\s)',text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSLiyIDcuASa",
        "outputId": "690abe16-b633-42c8-df01-732b5a4296cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', ' ', 'everyone', ' ', 'my', ' ', 'name', ' ', 'is', ' ', 'something', ',', '', ' ', 'an', ' ', 'I', '.', '', ' ', 'Finding', ' ', 'it', ' ', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REMOVING WHITESPACES OR NOT\n",
        "\n",
        "When developing a simple tokenizer, whether we should encode whitespaces as separate characters or just remove them depends on our application and its requirements. Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing). Here, we remove whitespaces for simplicity and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme that includes whitespaces.\n",
        "\n",
        "The tokenization scheme we devised above works well on the simple sample text. Let's modify it a bit further so that it can also handle other types of punctuation, such as question marks, quotation marks, and the double-dashes we have seen earlier in the first 100 characters of Edith Wharton's short story, along with additional special characters:"
      ],
      "metadata": {
        "id": "axH2MpUgvG-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--)', text)\n",
        "print(result)\n",
        "# result = [item.strip() for item in result if item.strip()]\n",
        "# print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjzB0nS7usu0",
        "outputId": "3318ada8-2467-4174-ecca-76b8302eb32b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', ' world', '.', ' Is this', '--', ' a test', '?', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',book)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWWf_U4GwtKT",
        "outputId": "b8de201f-d25a-4724-d1b1-be3c67fbef7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(preprocessed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYIuFk-SxqZE",
        "outputId": "fcb8377e-593f-4c55-f0c0-bcef20a43cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4690"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating token **IDs**"
      ],
      "metadata": {
        "id": "5_T0T564yG-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this creates a list of all unique tokens and sort them alphabetiically to determine the vocab size\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_LrEryEyBnY",
        "outputId": "60b12b3b-ab0c-40f9-a1c1-20a5c762de21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# enumerate(all_words)"
      ],
      "metadata": {
        "id": "FWXzV9wdymDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "wUrW33H3yTa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LnytFp-6y3se",
        "outputId": "a8301c60-0c20-4aa5-d416-24db3991c675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement a complete tokenizer class in Python.\n",
        "\n",
        "The class will have an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary.\n",
        "\n",
        "In addition, we implement a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text.\n",
        "\n",
        "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
        "\n",
        "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
        "\n",
        "Step 3: Process input text into token IDs\n",
        "\n",
        "Step 4: Convert token IDs back into text\n",
        "\n",
        "Step 5: Replace spaces before the specified punctuation"
      ],
      "metadata": {
        "id": "IXfrOiRezL1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self,vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self,text):\n",
        "      preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "      preprocessed = [\n",
        "          item.strip() for item in preprocessed if item.strip()\n",
        "      ]\n",
        "      ids = [self.str_to_int[s] for s in preprocessed]\n",
        "      return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "R0pFbJcjy5Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\" his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHX46fuHzih3",
        "outputId": "ab5ebaab-25e5-4251-bd26-03bad24095a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[549, 1042, 116, 7, 1, 73, 297, 585, 2, 850, 498, 1016, 866, 988, 1059, 722, 697, 769, 2, 1083, 1051, 9, 239, 53, 359, 2, 970]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)\n",
        "\n",
        "#here we need to undersrand that this tokenizer class is very  limited on vocab size and if you give an out of vocabulary word to this tokenizer class it'll throw an error\n",
        "#hence we need to develop a tokenizer that atleast doesnot throw an error if cannot the encode the word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "1Ce4OSDp0Q2o",
        "outputId": "899f2a1f-4a45-4abe-a794-9af9c5ecde85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'his unaccountable abdication.\" Of course it\\' s going to send the value of my picture\\' way up ; but I don\\' t'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADDING SPECIAL CONTEXT TOKENS\n",
        " we will modify the vocabulary and tokenizer we implemented in the previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and <|endoftext|>\n",
        "\n",
        "We can modify the tokenizer to use an <|unk|> token if it encounters a word that is not part of the vocabulary.\n",
        "\n",
        "Furthermore, we add a token between unrelated texts.\n",
        "\n",
        "For example, when training GPT-like LLMs on multiple independent documents or books, it is common to insert a token before each document or book that follows a previous text source\n",
        "the token will be <|endoftext|>\n"
      ],
      "metadata": {
        "id": "FKj_Lgij8GqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessed"
      ],
      "metadata": {
        "id": "dqo-TZMM0zDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_tokens = sorted(list(set(preprocessed)))\n",
        "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])"
      ],
      "metadata": {
        "id": "d_bxka948YH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_words)\n",
        "all_words[1130]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KCjWUi7X8yjQ",
        "outputId": "e99106cc-4cd0-4550-fc90-7dce6b748a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|endoftext|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {s:i for i,s in enumerate(all_words)}\n"
      ],
      "metadata": {
        "id": "9B4eOYsL8zh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab"
      ],
      "metadata": {
        "id": "A8FJNgqN9MLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "    def encode(self,text):\n",
        "      preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "      preprocessed = [\n",
        "          item.strip() for item in preprocessed if item.strip()\n",
        "      ]\n",
        "      preprocessed = [\n",
        "          item if item in self.str_to_int\n",
        "          else \"<|unk|>\" for item in preprocessed\n",
        "      ]\n",
        "      ids = [self.str_to_int[s] for s in preprocessed]\n",
        "\n",
        "      return ids\n",
        "\n",
        "    def decode(self,ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "5G0xlKSz9NCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the india.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YgZ7GnK9xr-",
        "outputId": "3d7af99a-95c0-4f0f-e230-239e01cb0dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the india.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "2IRWJygS-LRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VQLK4p3g-R8s",
        "outputId": "465972a5-bc03-4f93-f7f1-818f1eefe87b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[BOS] (beginning of sequence): This token marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
        "\n",
        "[EOS] (end of sequence): This token is positioned at the end of a text, and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one article ends and the next one begins.\n",
        "\n",
        "[PAD] (padding): When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the [PAD] token, up to the length of the longest text in the batch."
      ],
      "metadata": {
        "id": "7fUPaiEh-wXM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MItjb6H--V_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Byte Pair encoding\n",
        "#####So essentially we had 3 types of tokenization techniques - word based, character based and sub-word based\n",
        "we have already seen word based above, now bpe is an algorithm that is for sub-word based tokenization\n",
        "EVen in GPT architecture, BPE is used for tokenization\n",
        "Also the algorithm being complex, we'll use a premade library called tiktoken for this task"
      ],
      "metadata": {
        "id": "EllA9Pfc-71F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HGYgSE2_TYO",
        "outputId": "8c104a3a-3013-44f9-b9b7-6c84ca4c3938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ44aUQS_WTb",
        "outputId": "f90dfdcf-1a75-43a8-c836-f14c5a7cc528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#so remember here, at exactly this step before we had extracted the raw_text, created a vocabulary out of it that contained the, wrote the encode and decode methods\n",
        "#here in tiktoken gpt has already taken the text, applied the bpe algo and created the subswords\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "cD1wq65h_cI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqWhhQz1AAXd",
        "outputId": "f49f367c-e0ad-4d3f-a993-440c44aa0f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.decode(integers)\n",
        "print(text)\n",
        "\n",
        "#You see how the tiktoken is able to encode someunknownplace as well even though this combined is not even a word/place in the english vocabulary\n",
        "#also, endoftext is the last token in the vocab used in gpt - 50256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rORcbntQAFTW",
        "outputId": "a244379a-ef14-4e4b-c4cf-00af3f81d6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "integers = tokenizer.encode(\"Akwirw ier\")\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)\n",
        "\n",
        "##any gibberish word is converted to a certain set of token ids - power of tiktoken/bpe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBze9tfYAKbS",
        "outputId": "911999ec-fe06-49ec-e423-74ecd166cc13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n",
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encodings = {\n",
        "    \"gpt2\": tiktoken.get_encoding(\"gpt2\"),\n",
        "    \"gpt3\": tiktoken.get_encoding(\"p50k_base\"),  # Commonly associated with GPT-3 models\n",
        "    \"gpt4\": tiktoken.get_encoding(\"cl100k_base\")  # Used for GPT-4 and later versions\n",
        "}\n",
        "\n",
        "# Get the vocabulary size for each encoding\n",
        "vocab_sizes = {model: encoding.n_vocab for model, encoding in encodings.items()}\n",
        "\n",
        "# Print the vocabulary sizes\n",
        "for model, size in vocab_sizes.items():\n",
        "    print(f\"The vocabulary size for {model.upper()} is: {size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wNXu7vxAiV7",
        "outputId": "9f7676a6-442f-4e3b-a64c-db4a499eb09e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary size for GPT2 is: 50257\n",
            "The vocabulary size for GPT3 is: 50281\n",
            "The vocabulary size for GPT4 is: 100277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-xCH5AdOA0SU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}